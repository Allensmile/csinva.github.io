---
layout: notes_without_title
section-type: notes
title: interpretability
category: blog
---


# interpretability
**chandan singh** 
*last updated jul 20, 2018*

---

Modern machine learning models, particularly deep neural networks, are difficult to understand and explain. This has consequences when these networks are used to make critical decisions. Thus it is important to develop methods that can interpret these highly complex models.

As machine-learning models enter into high-stakes fields, interpretability will become increasingly important. Some of my recent work has focused on addressing this problem.


*"Interpretable machine learning: definitions, methods, and applications", <a color="#219AB3" href="https://arxiv.org/abs/1901.04592"> Murdoch, Singh, Kumbier, Abbasi-Asl, & Yu, 2019, arXiv</a>*

In this work, we aim to define what interpretability concretely means in the context of machine learning.
 